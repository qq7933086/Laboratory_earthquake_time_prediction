{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 2 Data wrangling","metadata":{}},{"cell_type":"markdown","source":"## 2.2 Introduction","metadata":{}},{"cell_type":"markdown","source":"This step focuses on collecting your data, organizing it, and making sure it's well defined.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Recap Of Data Science Problem","metadata":{}},{"cell_type":"markdown","source":"The purpose of this data science project is to predict time to failure given each small segment of acoustic signal using a data-driven model","metadata":{}},{"cell_type":"markdown","source":"## 2.3 Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from pandas_profiling import ProfileReport\nimport numpy as np\nimport glob, os\npd.set_option(\"display.precision\", 8)\npd.options.display.max_rows = 99\nfrom scipy import signal\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import acf\n%matplotlib inline  ","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:33:41.156999Z","iopub.execute_input":"2022-08-22T19:33:41.158043Z","iopub.status.idle":"2022-08-22T19:33:42.344584Z","shell.execute_reply.started":"2022-08-22T19:33:41.157957Z","shell.execute_reply":"2022-08-22T19:33:42.343409Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Objectives","metadata":{}},{"cell_type":"markdown","source":"There are some fundamental questions to resolve in this notebook before you move on.\n\n* Do you think you may have the data you need to tackle the desired question?\n    * Have you identified the required target value?\n    * Do you have potentially useful features?\n* Do you have any fundamental issues with the data?\n* Do your column names correspond to what those columns store?\n    * Check the data types of your columns. Are they sensible?\n    * Calculate summary statistics for each of your columns, such as mean, median, mode, standard deviation, range, and number of unique values. What does this tell you about your data? What do you now need to investigate?","metadata":{}},{"cell_type":"markdown","source":"# 2.5 Load the training data","metadata":{}},{"cell_type":"markdown","source":"## 2.5.0 Check complete data every 100 records","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_df = pd.read_csv(os.path.join(PATH,'train.csv'), dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ad_sample_df = train_df['acoustic_data'].values[::100]\ntrain_ttf_sample_df = train_df['time_to_failure'].values[::100]\n\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(16, 12))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5.1 Check a small section of data","metadata":{}},{"cell_type":"code","source":"filename = '../input/LANL-Earthquake-Prediction/train.csv'\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:33:48.279791Z","iopub.execute_input":"2022-08-22T19:33:48.280161Z","iopub.status.idle":"2022-08-22T19:33:48.285969Z","shell.execute_reply.started":"2022-08-22T19:33:48.280133Z","shell.execute_reply":"2022-08-22T19:33:48.284631Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# df_tr_sec =  pd.read_csv(filename,nrows = 6e6)\nrecords = 10_000_000\ndf_tr_sec =  pd.read_csv(filename,nrows = records)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:33:48.998682Z","iopub.execute_input":"2022-08-22T19:33:48.999445Z","iopub.status.idle":"2022-08-22T19:33:52.308710Z","shell.execute_reply.started":"2022-08-22T19:33:48.999413Z","shell.execute_reply":"2022-08-22T19:33:52.307804Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_tr_sec.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-22T19:33:52.310167Z","iopub.execute_input":"2022-08-22T19:33:52.310460Z","iopub.status.idle":"2022-08-22T19:33:52.333730Z","shell.execute_reply.started":"2022-08-22T19:33:52.310433Z","shell.execute_reply":"2022-08-22T19:33:52.332346Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_tr_sec.describe()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-22T16:52:31.035333Z","iopub.execute_input":"2022-08-22T16:52:31.035822Z","iopub.status.idle":"2022-08-22T16:52:32.239618Z","shell.execute_reply.started":"2022-08-22T16:52:31.035775Z","shell.execute_reply":"2022-08-22T16:52:32.238281Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_tr_sec.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-08-22T16:52:33.018983Z","iopub.execute_input":"2022-08-22T16:52:33.019443Z","iopub.status.idle":"2022-08-22T16:52:33.031610Z","shell.execute_reply.started":"2022-08-22T16:52:33.019404Z","shell.execute_reply":"2022-08-22T16:52:33.030272Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1,1,figsize=(16, 12), dpi=100 )\nax2 = ax1.twinx()\nax1.plot(df_tr_sec['acoustic_data'], 'g.')\nax2.plot(df_tr_sec['time_to_failure'], 'b.')\n\n\nax1.set_ylabel('Signal', color='g')\nax2.set_ylabel('Time to failure', color='b')\nax2.grid()\nplt.title(f'The first {records /1000}k data points');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tr_sec.plot('time_to_failure','acoustic_data')\nplt.gca().invert_xaxis()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# time2fail_np = df_tr_sec['time_to_failure'].values\n# # dt_np = time2fail_np[0:-1] - time2fail_np[1:] \ndt = pd.Series(-df_tr_sec['time_to_failure'].diff(), name='dt')\nprint(dt.value_counts())\nplt.hist(dt.sort_values()[1:])# remove the time difference between two segement, which is about -11\nplt.title('Histogram of time sampling')\nplt.ylabel('Count')\nplt.yscale('log')\nplt.xlabel('time sampling');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`acoustic_data` distribute unevenly.\n\nFrom `time_to_failure`, the equipment may work a while to acquire signal at high frequency (1.1e-9 s), and then take a rest (1e-3 s)before next aquisition.\n\nThe sampling rate is not a constant, and the distribution of time step is bimodal with one large timestep and a small timestep.\n\nTry resample with the large timestep later.\n\n","metadata":{}},{"cell_type":"markdown","source":"## 2.5.2 Truncate training data for each earthquake event","metadata":{}},{"cell_type":"code","source":"# using chunk to manipulate the raw train data\nfilenum = 0\nj = 0\nprevious_df = pd.DataFrame()\ntotal_missing = np.array([0,0])\ntotal_records = 0\nmin_time = 100000\n\nfor chunk in pd.read_csv(filename, chunksize = 1e7):\n    # count missing value in each column\n\n    total_missing += chunk.isna().sum().to_numpy()\n    total_records += len(chunk)\n    min_time = np.array([chunk.iloc[-1,1], min_time]).min()\n    \n    j += 1\n    dt_df = chunk[['time_to_failure']].shift(fill_value = chunk.iloc[0]['time_to_failure']) - chunk[['time_to_failure']]\n    # find the last index of an earthquake event\n    idx = dt_df[dt_df['time_to_failure'] < 0].index\n    \n    if len(idx) == 0:\n        previous_df = pd.concat([previous_df,chunk])\n    else:\n        prev_i = chunk.index[0]\n        for i in idx:\n            # save each earthquake event in a pickle file\n            pd.concat([previous_df, chunk.loc[prev_i:i, :]]).to_pickle('D:/DataScience/traindata/train_sec_' + str(filenum) + '.pkl')\n            filenum += 1\n            prev_i = i\n        previous_df = chunk.loc[i:, :]\n\n# The last earthquake file    \nprevious_df.to_pickle('D:/DataScience/traindata/train_sec_' + str(filenum) + '.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"nan for two variables: \", total_missing)\nprint(\"Total records: \", total_records)\nprint(\"Last time of recording: \", min_time)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5.3 Check tuncated data","metadata":{}},{"cell_type":"code","source":"def plot_train_segment(fileobject):\n    df = pd.read_pickle(fileobject)\n    print(df.columns)\n    print('Records number:',len(df))\n    df.plot(x = 'time_to_failure', y = 'acoustic_data')\n    \n    plt.gca().invert_xaxis()\n    plt.title(fileobject[31:-4])\n    plt.savefig(\"../images/\" + fileobject[25:-4] + \".jpg\")\n    #     plt.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pathlist = ['D:/DataScience/traindata/train_sec_' + str(num) + '.pkl' for num in range (0,17)]\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the training segment and save figure\nfor path in pathlist[4:6]:\n    plot_train_segment(path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tr_end = pd.read_pickle(pathlist[-1])\ndf_tr_end.iloc[-5:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`train_sec_16` is not complete, i.e., it does not records the failure event","metadata":{}},{"cell_type":"markdown","source":"## 2.5.4 Pandas profiling for a training segment","metadata":{}},{"cell_type":"code","source":"df_tr_sec = pd.read_pickle(pathlist[7])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile = ProfileReport(df_tr_sec, title=\"Pandas Profiling Report of the Training Data 6\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# profile.to_widgets()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"profile.to_file(\"../report/training_data_report_6.html\")","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5.5 Resampling the training data with 1 ms sampling rate","metadata":{}},{"cell_type":"code","source":"# power spectrum density (square of amplitude spectrum)\ndef psd(input_signal):\n    f, Pxx_den = signal.periodogram(input_signal, fs = 1/1.1e-9)\n    f_dom = f[np.argmax(Pxx_den)]\n    return f, Pxx_den, f_dom","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iqr(X):\n    Q1 = X.quantile(0.25)\n    Q3 = X.quantile(0.75)\n    iqr_X = Q3 - Q1\n    return iqr_X","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_feature(X):\n    mean_X = X.mean()\n    median_X = X.median()\n    std_X = X.std()\n    iqr_X = iqr(X)\n    range_X = np.max(X) - np.min(X)\n#     min_X = np.min(X)\n#     max_X = np.max(X)\n#     f, Pxx_den, f_dom_X = psd(X)\n    return [mean_X, median_X, std_X, iqr_X, range_X]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_lag_feature(X, periods= 0):\n    feature_list = []\n    for period in range(1,periods):\n        feature_list.extend(create_feature(X.diff(periods))) \n    return feature_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_feature_by_smplrt(df, precision):\n    if precision == 4:\n        col_name = 'ttf_ms'\n    elif precision == 6:\n        col_name = 'ttf_us'\n        \n    # Create feature using statistics\n    df[col_name] = df['time_to_failure'].round(precision).astype('str')\n    df_agg = df.groupby(col_name)['acoustic_data'].agg(create_feature)\n    df_agg = pd.DataFrame(df_agg.tolist(), index= df_agg.index, columns = [ 'mean','median','std','iqr','range'])\n    \n    # prepare feature names of lagging data in time domain\n    col_name_by_lag = [i + '_lag' for i in ['mean','median','std','iqr','range']]\n    col_names = []\n    for j in range(1,11):\n        col_names.extend([name + '_'+ str(j) for name in col_name_by_lag])\n        \n    # Create feature using time lagging on acoustic data\n    df_lag_agg = df.groupby(col_name)['acoustic_data'].agg(create_lag_feature, periods = 11)\n    df_lag_agg = pd.DataFrame(df_lag_agg.tolist(), index= df_lag_agg.index, columns =col_names)\n    \n    # Merge features in time domains\n    df_time = df_agg.merge(df_lag_agg, how = 'left',left_index = True, right_index = True)\n\n    # Frequency domain statistics in sampling signal\n    df_psd = df.groupby(col_name)['acoustic_data'].agg(psd)\n    df_psd = pd.DataFrame(df_psd.tolist(), index= df_psd.index, columns = ['freq', 'psd','f_dom'])\n    \n    # prepare feature names in frequency domain\n    fsecs = ['f_sec' + str(num) for num in range(1,11)] # variable names\n    df_f = pd.DataFrame(columns = fsecs)\n    for index, row in df_psd.iterrows():\n        f_dict = dict()\n        freq_sec = np.linspace(0,1e8,11)\n        PSD = row['psd']\n        freq = row['freq']\n        area = PSD[freq < 1e8].sum()\n\n        for idx, fsec in enumerate(fsecs):\n            row_filter = (freq >= freq_sec[idx]) & (freq < freq_sec[idx+1])\n            pzone = PSD[row_filter].sum()    \n            f_dict[fsec] = pzone / area\n        temp = pd.DataFrame(f_dict,index =[index] )\n        df_f = df_f.append(temp) \n    df_psd = df_psd.merge(df_f, how = 'left', left_index = True, right_index = True)\n    \n    df_new_smplrt = df_time.merge(df_psd, how = 'left', left_index = True, right_index = True).reset_index()\n    df_new_smplrt[col_name] = df_new_smplrt[col_name].astype('float')\n    return df_new_smplrt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.1 Create feature using statistics on 1 ms groups","metadata":{}},{"cell_type":"code","source":"df_tr_sec['ttf_round'] = df_tr_sec['time_to_failure'].round(4).astype('str')\ndf_agg = df_tr_sec.groupby('ttf_round')['acoustic_data'].agg(create_feature)\ndf_agg = pd.DataFrame(df_agg.tolist(), index= df_agg.index, columns = [ 'mean','median','std','iqr','diff'])\n# df_agg.reset_index(inplace = True)\n\ndf_agg.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.2 Create feature using time lagging on acoustic data","metadata":{}},{"cell_type":"code","source":"# prepare feature names of lagging data in time domain\ncol_name_by_lag = [i + '_lag' for i in ['mean','median','std','iqr', 'diff']]\n\ncol_names = []\nfor j in range(1,11):\n    col_names.extend([name + '_'+ str(j) for name in col_name_by_lag])\ncol_names[:10]","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tr_sec['ttf_round'] = df_tr_sec['time_to_failure'].round(4).astype('str')\ndf_lag_agg = df_tr_sec.groupby('ttf_round')['acoustic_data'].agg(create_lag_feature, periods = 11)\ndf_lag_agg = pd.DataFrame(df_lag_agg.tolist(), index= df_lag_agg.index, columns =col_names)\ndf_lag_agg.head(5).T","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.3 Time domain features compilation","metadata":{}},{"cell_type":"code","source":"df_tr_agg = df_agg.merge(df_lag_agg, how = 'left',left_index = True, right_index = True)\ndf_tr_agg.head(5).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.4 Frequency domain in 1 ms sampling signal","metadata":{}},{"cell_type":"markdown","source":"Plot periodogram (square of amplitude spectrum)","metadata":{}},{"cell_type":"code","source":"row1 = df_tr_sec['ttf_round'] == df_tr_sec['ttf_round'].unique()[1]\nf1, Pxx_den1, f_dom1 = psd(df_tr_sec.loc[row1,'acoustic_data'])\n\nrow2 = df_tr_sec['ttf_round'] == df_tr_sec['ttf_round'].unique()[2]\nf2, Pxx_den2, f_dom2 = psd(df_tr_sec.loc[row2,'acoustic_data'])\n\nfig, ax_list = plt.subplots(2,1,figsize=(16, 12), dpi=100 )\nax = ax_list[0]\nax.plot(f1,Pxx_den1)\nax.set_ylabel('Power spectral density')\nax.set_xlabel('Frequency (Hz)')\nax.set_title('time to failure: ' + df_tr_sec['ttf_round'].unique()[1] + ' sec' )\n\nax = ax_list[1]\nax.plot(f2,Pxx_den2)\nax.set_ylabel('Power spectral density')\nax.set_xlabel('Frequency (Hz)')\nax.set_title('time to failure: ' + df_tr_sec['ttf_round'].unique()[2] + ' sec' );","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The frequency component mainly distribute in 1e8 Hz. Divide this frequency interval to 10 buckets, calculate the probability to create some features","metadata":{}},{"cell_type":"code","source":"df_psd = df_tr_sec.groupby('ttf_round')['acoustic_data'].agg(psd)\ndf_psd = pd.DataFrame(df_psd.tolist(), index= df_psd.index, columns = ['freq', 'psd','f_dom'])\ndf_psd.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fsecs = ['f_sec' + str(num) for num in range(1,11)] # variable names\ndf_f = pd.DataFrame(columns = fsecs)\nfor index, row in df_psd.iterrows():\n#     print(type(row))\n    f_dict = dict()\n    freq_sec = np.linspace(0,1e8,11)\n    PSD = row['psd']\n    freq = row['freq']\n    area = PSD[freq < 1e8].sum()\n\n    for idx, fsec in enumerate(fsecs):\n        row_filter = (freq >= freq_sec[idx]) & (freq < freq_sec[idx+1])\n        pzone = PSD[row_filter].sum()    \n        f_dict[fsec] = pzone / area\n    temp = pd.DataFrame(f_dict,index =[index] )\n#     print(temp)\n    df_f = df_f.append(temp) \ndf_psd = df_psd.merge(df_f, how = 'left', left_index = True, right_index = True)\ndf_psd.head().T","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.5 Merge time- and frequency-domain features\nmerge time-lag features `df_lag_all` with frequency-domain features `df_psd`","metadata":{}},{"cell_type":"code","source":"df_agg_all = df_tr_agg.merge(df_psd, how = 'left', left_index = True, right_index = True).reset_index()\ndf_agg_all['ttf_round'] = df_agg_all['ttf_round'].astype('float')\ndf_agg_all.head(5).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_lag = df_agg_all.corr()\nmask = np.triu(np.ones_like(corr_lag, dtype=bool))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(16, 16))\nax = sns.heatmap(corr_lag, mask=mask, cmap=cmap, vmax=.3, center=0,\n                 square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('1 ms sampling rate');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_agg_all.hist(figsize=(16,16))\nplt.subplots_adjust(hspace=0.5);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Q: why median and iqr only have a precision of 1\n\nBecause `acoustic_data` is int type.","metadata":{}},{"cell_type":"code","source":"df_ms = create_feature_by_smplrt(df_tr_sec, 4)\ndf_ms.head(5).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.5.5.6 Compare resampling data with raw data in periodogram\n","metadata":{}},{"cell_type":"code","source":"df_ms.ttf_ms.diff().mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# psd()\n# def psd(input_signal):\n#     f, Pxx_den = signal.periodogram(input_signal, fs = 1/1.1e-9)\n#     f_dom = f[np.argmax(Pxx_den)]\n#     return f, Pxx_den, f_dom\nf_ms, Pxx_den_ms_mean = signal.periodogram(df_ms['mean'], fs = 1/0.00106)\n_, Pxx_den_ms_median = signal.periodogram(df_ms['median'], fs = 1/0.00106)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot periodogram of 1 ms sampling data","metadata":{}},{"cell_type":"code","source":"# plot\nfig, ax_list = plt.subplots(2,1,figsize=(16, 12), dpi=100 )\nax = ax_list[0]\nax.plot(f_ms, Pxx_den_ms_mean)\nax.set_ylabel('Power spectral density')\nax.set_xlabel('Frequency (Hz)')\nax.set_title('1 ms sampling data by mean')\n\nax = ax_list[1]\nax.plot(f_ms, Pxx_den_ms_median)\nax.set_ylabel('Power spectral density')\nax.set_xlabel('Frequency (Hz)')\nax.set_title('1 ms sampling data by median' );","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot periodogram for raw data","metadata":{}},{"cell_type":"code","source":"w = np.linspace(1, 500, 100_000)\npgram = signal.lombscargle(df_tr_sec['time_to_failure'], df_tr_sec['acoustic_data'], w)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax_w = plt.subplots()\nax_w.plot(w/2/np.pi, pgram)\nax_w.set_xlabel('Frequency (Hz)')\nax_w.set_ylabel('Amplitude')\nax_w.set_title('Raw data periodogram' );\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"range_cols = [col for col in df_ms.columns if 'range' in col]\ndf_ms[range_cols].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_lag = df_ms.corr()\nmask = np.triu(np.ones_like(corr_lag, dtype=bool))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(16, 16))\nax = sns.heatmap(corr_lag, mask=mask, cmap=cmap, vmax=.3, center=0,\n                 square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('1 ms sampling rate');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5.6 Resampling the training data with 1 us sampling rate","metadata":{}},{"cell_type":"code","source":"df_us = create_feature_by_smplrt(df_tr_sec, 6)\ndf_us.head(5).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_lag = df_us.corr()\nmask = np.triu(np.ones_like(corr_lag, dtype=bool))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(16, 16))\nax = sns.heatmap(corr_lag, mask=mask, cmap=cmap, vmax=.3, center=0,\n                 square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('1 us sampling rate');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Compare periodogram of 1 us sampling data and original data","metadata":{}},{"cell_type":"markdown","source":"## 2.5.7 Rolling window to get statistic features","metadata":{}},{"cell_type":"code","source":"windows = [10, 50, 100, 500]\nfor i in range(len(windows)):\n    df_tr_sec[\"rolmean_\" + str(windows[i])] = df_tr_sec['acoustic_data'].rolling(windows[i], center = True).mean()\n    df_tr_sec[\"rolstd_\" + str(windows[i])] = df_tr_sec['acoustic_data'].rolling(windows[i], center = True).std()\ndf_tr_sec.info()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:34:14.197206Z","iopub.execute_input":"2022-08-22T19:34:14.197577Z","iopub.status.idle":"2022-08-22T19:34:16.072334Z","shell.execute_reply.started":"2022-08-22T19:34:14.197547Z","shell.execute_reply":"2022-08-22T19:34:16.071435Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Choose a window size\nfig, axs = plt.subplots(len(windows),1,figsize=(16, len(windows) * 5))\nfig.subplots_adjust(hspace = .5)\naxs = axs.ravel()\nfor i in range(len(windows)):\n    roll_smpl = df_tr_sec.loc[1_100_000:1_110_000]\n    axs[i].plot(roll_smpl.index.values, roll_smpl['rolmean_' + str(windows[i])].values, color = 'b',label = 'rolmean_' + str(windows[i]))\n#         axs[i].plot(df_tr_sec['rolmean'] + 2 * df_tr_sec['rolstd'], 'r:',df_tr_sec['rolmean'] - 2 * df_tr_sec['rolstd'], 'r:',label = 'rolling_mean + std')\n    axs[i].fill_between(roll_smpl.index.values,\n                        roll_smpl['rolmean_' + str(windows[i])] - roll_smpl['rolstd_' + str(windows[i])], \n                        roll_smpl['rolmean_' + str(windows[i])] + roll_smpl['rolstd_' + str(windows[i])],\n                        facecolor='lightgreen', alpha = 0.5, label='rolstd_' + str(windows[i]))\n    axs[i].legend()\n    axs[i].set_xlabel('index')\n    axs[i].set_ylabel('Acoustic signal')\n    axs[i].set_title(\"Rolling window = %.i\" % windows[i] )\n#     axs[i].set_ylim((-500, 500))","metadata":{"execution":{"iopub.status.busy":"2022-08-22T16:53:20.230604Z","iopub.execute_input":"2022-08-22T16:53:20.231674Z","iopub.status.idle":"2022-08-22T16:53:20.993211Z","shell.execute_reply.started":"2022-08-22T16:53:20.231633Z","shell.execute_reply":"2022-08-22T16:53:20.991818Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"w =50\narfun = lambda x: x.autocorr()\ndf_tr_sec[\"rolmean_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).mean()\ndf_tr_sec[\"rolstd_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).std()\ndf_tr_sec[\"rolkurt_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).kurt()\ndf_tr_sec[\"rolskew_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).skew()\ndf_tr_sec[\"rolquantile_25_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).quantile(0.25)\ndf_tr_sec[\"rolquantile_50_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).median()\ndf_tr_sec[\"rolquantile_75_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).quantile(0.75)\ndf_tr_sec[\"rolIQR_\" + str(w)] = df_tr_sec[\"rolquantile_75_\" + str(w)] - df_tr_sec[\"rolquantile_25_\" + str(w)]\ndf_tr_sec[\"rolmin_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).min()\ndf_tr_sec[\"rolmax_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).max()\ndf_tr_sec[\"rolsum_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).sum()\ndf_tr_sec[\"rolautocorr_\" + str(w)] = df_tr_sec['acoustic_data'].rolling(w, center = True).apply(arfun)\n\n# create auto correlation features with lag\n# for col in df_tr_sec.columns[2:]:\n#     for n in range(11):\n#         df_tr_sec[col + \"autocorr_lag_\" + str(n)] = df_tr_sec[col].autocorr(lag = n)  \ndf_tr_sec.info()\n# save the data\ndf_tr_sec.to_pickle('./training_section.pickle')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-22T19:34:22.987770Z","iopub.execute_input":"2022-08-22T19:34:22.988290Z","iopub.status.idle":"2022-08-22T20:23:08.657522Z","shell.execute_reply.started":"2022-08-22T19:34:22.988260Z","shell.execute_reply":"2022-08-22T20:23:08.655833Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# save the data\ndf_tr_sec.to_pickle('./training_section.pickle')","metadata":{"execution":{"iopub.status.busy":"2022-08-22T20:29:12.916095Z","iopub.execute_input":"2022-08-22T20:29:12.916474Z","iopub.status.idle":"2022-08-22T20:29:14.833001Z","shell.execute_reply.started":"2022-08-22T20:29:12.916436Z","shell.execute_reply":"2022-08-22T20:29:14.831491Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_tr_sec.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_tr_sec.iloc[25:-24]['acoustic_data']\nplot_acf(df_tr_sec.iloc[25:-24]['acoustic_data'], lags = 10)\n# plot_acf（df_tr_sec.iloc[25:-24]['acoustic_data'],lags=10） #rolmean_50","metadata":{"execution":{"iopub.status.busy":"2022-08-22T20:35:27.540493Z","iopub.execute_input":"2022-08-22T20:35:27.540888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acf(df_tr_sec.iloc[25:-24]['rolmean_50'], lags = 10)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr = df_tr_sec.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nf, ax = plt.subplots(figsize=(16, 16))\nax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                 square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title('rolling features');","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.6 Load the testing data","metadata":{}},{"cell_type":"code","source":"testfilepath = \"../../../rawdata/2ndCapstone/test/\"\nos.chdir(testfilepath)\ntestfiles = glob.glob(\"*.csv\")\nprint(testfiles[0:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_test_file(fileobject):\n    df = pd.read_csv(fileobject)\n    print(df.describe())\n    df.plot(y = 'acoustic_data')\n    plt.title(fileobject[0:-4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_te = pd.read_csv(testfiles[0])\ndf_te.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for testfile in testfiles[0:3]:\n    print(len(testfile))\n    plot_test_file(testfile)\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.7 Save data","metadata":{}},{"cell_type":"code","source":"# save the data to a new pickle file\ndf_ms.to_pickle('../data/train_ms_section.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_us.to_pickle('../data/train_us_section.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":".to_pickle('D:/DataScience/traindata/train_sec_' + str(filenum) + '.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.8 Summary","metadata":{}},{"cell_type":"markdown","source":"For the training data:\n* The data has zero missing values for both columns\n* The data has only two column ,`acoustic_data` in int type and `time_to_failure` in float type. `time_to_failure` is our target.\n* The data has 629,145,480 records, too large to operate.\n* `acoustic_data` is the acoustic emission signal (amplitude), consisting of many peaks and troughts. \n* `time_to_failure` decreases from a value to zero periodically. The data should be truncated based on it.\n* 17 segments were obtained after analyzing chunk data in the rawdata. \n* Each training segment has different time length\n* The first one only has about two seconds record to earthquake. The last one is not complete because the `time_to_failure` is 9.75 s, far from zero.\n* After `acoustic_data` reaches a extremly large event, the eqrthquake occurs soon in each training segment, and `time_to_failure` is less than 0.5s.\n\nFor the testing data:\n* Only one row in each file, which is the `acoustic_data`\n* Each segment contains 150k records, a small segment from a complete earthquake event. By comparision, records in training data is about 200 times of the test data. This may inspire us the feature extraction from the training data\n* Similar to the training data, the average `acoustic_data` of a test data are small, while some abnormal `acoustic_data` exists. These determines what a feature is.\n* Most test data may not contain the extremly large event with an absolute of amplitude over 1000. They contains peaks at a scale of several hundred amplitude","metadata":{}},{"cell_type":"markdown","source":"7/11/2022\nFor the training data:\n\n* Resampling using groupby on round precision = 4 for `time_to_failure`\n* Time lagging is tried on `acoustic_data` to create features. However, what time step is an optimum value?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}